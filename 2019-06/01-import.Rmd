---
title: "Import Democratic Candidate 2019 Data"
author: "Jane Doe"
output: github_document
---

```{r setup, include=FALSE}
# create image folder ----
if (!file.exists("figs/")) {
    dir.create("figs/")
}
# create data folder ----
if (!file.exists("data/")) {
    dir.create("data/")
}
knitr::opts_chunk$set(
    echo = TRUE, # show all code
    eval = FALSE,
    message = FALSE, 
    comment = "#> ",
    warning = TRUE,
    tidy = FALSE, # cleaner code printing
    size = "small",
    fig.path = "figs/") # smaller code
knitr::opts_knit$set(
    width = 78)
base::options(tibble.print_max = 25,
              tibble.width = 78)
```

**Rmarkdown:**

This is an R Markdown format used for publishing markdown documents. When you click the **Knit** button all R code chunks are run and a markdown file (`.md`) suitable for publishing to GitHub is generated.

**Including Code:**

You can include R code in the document to keep track of your work! Below we perform a search for Democratic presidential candidates during the debates in June of 2019.

# Motivation

This document describes how various data sources were downloaded for this project. The first night of the Democratic presidential debate held in Miami, Florida, on June 26, 2019.

## Packages

The packages are installed in the `00.1-inst-packages.R` script. We'll load the packages in the code chunk below. 

```{r 00.1-inst-packages.R, eval=TRUE}
library(gtrendsR)
library(maps)
library(ggplot2)
library(lettercase)
library(viridis)
library(pals)
library(scico)
library(ggrepel)
library(tidyverse)
library(skimr)
```

## The data 

- Google trend data from the [`gtrendsR`](https://github.com/PMassicotte/gtrendsR) package for R gives us access to Google search terms and trends. We're going to use this to import data on Google searches for the candidates before and after the night of the debates. 

```{r gtrendsR-dont-run, eval=FALSE}
# import Google data -----------------------------------------------------
# these have been imported using the 01.2-twitter-data.R file. Open it for more 
# details
# fs::dir_ls("data/raw/google-trends/")
google_data_path <- "data/raw/google-trends/"   # path to the data
google_data_files <- dir(google_data_path, 
                          pattern = "*.rds") # get file names
GoogleData <- google_data_files %>%
  # read in all the files, appending the path before the filename
  purrr::map(~ readr::read_rds(file.path(google_data_path, .)))
GTrendDems2020Night1G1 <- GoogleData[[1]]
GTrendDems2020Night1G2 <- GoogleData[[2]]
```

2) The [`rtweet`](https://rtweet.info/) package in R takes a little bit to get set up. Fortunately, we've written a tutorial [here](http://www.storybench.org/get-twitter-data-rtweet-r/) and the package has excellent documentation (see [here](https://rtweet.info/articles/auth.html) and [here](https://rtweet.info/articles/intro.html)).

```{r wiki-dont-run, eval=FALSE}
# import twitter data -----------------------------------------------------
# these have been imported using the 01.2-twitter-data.R file. Open it for more 
# details
# fs::dir_ls("data/raw/twitter/")
twitter_data_path <- "data/raw/twitter/"   # path to the data
twitter_data_files <- dir(twitter_data_path, 
                          pattern = "*Tweets.rds") # get file names
TwitterData <- twitter_data_files %>%
  # read in all the files, appending the path before the filename
  purrr::map(~ readr::read_rds(file.path(twitter_data_path, .))) %>% 
  reduce(rbind)
# TwitterData

twitter_data_path <- "data/raw/twitter/"   # path to the data
twitter_users_data_files <- dir(twitter_data_path, 
                                pattern = "*Users.rds") # get file names
TwitterUsersData <- twitter_users_data_files %>%
  # read in all the files, appending the path before the filename
  purrr::map(~ readr::read_rds(file.path(twitter_data_path, .))) %>% 
  reduce(rbind)
# TwitterUsersData
```

3) There are data from voters on how they felt about each candidate going into the debates stored in a Google Sheet [here](http://bit.ly/2YEVASu) that we've accessed using the [`googlesheets4`](https://googlesheets4.tidyverse.org/) package in R (*you will need to copy this sheet into your Google drive to get this data set*). Another option is to use the [`datapasta`](https://cran.r-project.org/web/packages/datapasta/README.html) package and copy + paste these data into R. 

```{r 538}
# import 538 --------------------------------------------------------------
# fs::dir_ls("data/raw/538/")
GSheetCand538Fav <- readr::read_csv("data/raw/538/2019-07-06-Cand538Fav.csv")
```


4) There is a [Wikipedia](https://en.wikipedia.org/wiki/2020_Democratic_Party_presidential_debates_and_forums) page dedicated to the debates. We will be extracting the tables with airtime for candidate using the [`xml2`](https://cran.r-project.org/web/packages/xml2/index.html) and [`rvest`](https://rvest.tidyverse.org/) packages. 

```{r wikipedia-tables}
# import wikipedia --------------------------------------------------------
wiki_data_path <- "data/raw/wikipedia/"   # path to the data
wiki_data_files <- dir(wiki_data_path, 
                          pattern = "*.csv") # get file names
WikiData <- wiki_data_files %>%
  # read in all the files, appending the path before the filename
  purrr::map(~ readr::read_csv(file.path(wiki_data_path, .)))
WikiDemAirTime01Raw <- WikiData[[1]]
# WikiDemAirTime01Raw
WikiDemAirTime02Raw <- WikiData[[2]]
# WikiDemAirTime02Raw
WikiPollCriterionRaw <- WikiData[[3]]
# WikiPollCriterionRaw
```


## The candidates 

There were ten candidates in the first night of the debates, and ten in the second night. We have created a vector with all of them listed alphabetically below.

```{r dem_cand_2020}
dem_candidates <- c("Amy Klobuchar",
                "Beto O’Rourke",
                "Bill de Blasio",
                "Cory Booker",
                "Elizabeth Warren",
                "Jay Inslee",
                "John Delaney",
                "Julián Castro",
                "Tim Ryan",
                "Tulsi Gabbard")
writeLines(dem_candidates)
```




